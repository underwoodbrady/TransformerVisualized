<footer class="bg-neutral-300 px-4 border-t-2 border-black">
    <h4>References</h4>
    <ul class=" list-disc ml-2">
        <li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1), 1929-1958.</li>
        <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</li>
        <li>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.</li>
        <li>Andrej Karpathy. (2023) Let's build GPT: from scratch, in code, spelled out. Youtube</li>
        <li>Elliot Arledge, freeCodeCamp.org. (2023) Create a Large Language Model from Scratch with Python â€“ Tutorial. Youtube</li>
        <li>3Blue1Brown (2024) But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning. Youtube</li>
    </ul>
</footer>